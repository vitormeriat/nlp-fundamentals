{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector transformations in Gensim**\n",
    "\n",
    "Now that we know what vector transformations are, let's get used to creating them, and using them. We will be performing these transformations with Gensim, but even scikit-learn can be used. We'll also have a look at scikit-learn's approach later on.\n",
    "\n",
    "Let's create our corpus now. We discussed earlier that a corpus is a collection of documents. In our examples, each document would just be one sentence, but this is obviously not the case in most real-world examples we will be dealing with. We should also note that once we are done with preprocessing, we get rid of all punctuation marks - as for as our vector representation is concerned, each document is just one sentence.\n",
    "\n",
    "Of course, before we start, be sure to install Gensim. Like spaCy, pip or conda is the best way to do this based on your working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We performed very similar preprocessing when we introduced spaCy. What do our documents look like now?\n",
    "documents = [u\"Football club Arsenal defeat local rivals this weekend.\", \n",
    "             u\"Weekend football frenzy takes over London.\", \n",
    "             u\"Bank open for take over bids after losing millions.\", \n",
    "             u\"London football clubs bid to move to Wembley stadium.\", \n",
    "             u\"Arsenal bid 50 million pounds for striker Kane.\", \n",
    "             u\"Financial troubles result in loss of millions for bank.\", \n",
    "             u\"Western bank files for bankruptcy after financial losses.\", \n",
    "             u\"London football club is taken over by oil millionaire from Russia.\", \n",
    "             u\"Banking on finances not working for Russia.\"]\n",
    "\n",
    "texts = []\n",
    "for document in documents:\n",
    "    doc = nlp(document)\n",
    "    text = [\n",
    "        w.lemma_\n",
    "        for w in doc\n",
    "        if not w.is_stop and not w.is_punct and not w.like_num\n",
    "    ]\n",
    "    texts.append(text)\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by whipping up a bag-of-words representation for our mini-corpus. Gensim allows us to do this very conveniently through its `dictionary` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arsenal': 0, 'club': 1, 'defeat': 2, 'football': 3, 'local': 4, 'rival': 5, 'weekend': 6, 'London': 7, 'frenzy': 8, 'take': 9, 'bank': 10, 'bid': 11, 'lose': 12, 'million': 13, 'open': 14, 'Wembley': 15, 'stadium': 16, 'Kane': 17, 'arsenal': 18, 'pound': 19, 'striker': 20, 'financial': 21, 'loss': 22, 'result': 23, 'trouble': 24, 'bankruptcy': 25, 'file': 26, 'western': 27, 'Russia': 28, 'millionaire': 29, 'oil': 30, 'banking': 31, 'finance': 32, 'work': 33}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 32 unique words in our corpus, all of which are represented in our dictionary with each word being assigned an index value. When we refer to a word's word_id henceforth, it means we are talking about the words integer-id mapping made by the dictionary.\n",
    "\n",
    "We will be using the `doc2bow` method, which, as the name suggests, helps convert our document to bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print our corpus, we'll have our bag of words representation of the documents we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1)],\n",
       " [(1, 1), (3, 1), (7, 1), (11, 1), (15, 1), (16, 1)],\n",
       " [(11, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
       " [(10, 1), (13, 1), (21, 1), (22, 1), (23, 1), (24, 1)],\n",
       " [(10, 1), (21, 1), (22, 1), (25, 1), (26, 1), (27, 1)],\n",
       " [(1, 1), (3, 1), (7, 1), (9, 1), (28, 1), (29, 1), (30, 1)],\n",
       " [(28, 1), (31, 1), (32, 1), (33, 1)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4538520228951382), (1, 0.2269260114475691), (2, 0.4538520228951382), (3, 0.1675032779320012), (4, 0.4538520228951382), (5, 0.4538520228951382), (6, 0.3106776504135697)]\n",
      "[(3, 0.2421296766697527), (6, 0.44909138478886224), (7, 0.32802654645398593), (8, 0.6560530929079719), (9, 0.44909138478886224)]\n",
      "[(10, 0.29019840161676663), (11, 0.29019840161676663), (12, 0.5803968032335333), (13, 0.3973019972146358), (14, 0.5803968032335333)]\n",
      "[(1, 0.29431054749542984), (3, 0.21724253258131515), (7, 0.29431054749542984), (11, 0.29431054749542984), (15, 0.5886210949908597), (16, 0.5886210949908597)]\n",
      "[(11, 0.24253562503633302), (17, 0.48507125007266605), (18, 0.48507125007266605), (19, 0.48507125007266605), (20, 0.48507125007266605)]\n",
      "[(10, 0.2615055248879334), (13, 0.35801943340074827), (21, 0.35801943340074827), (22, 0.35801943340074827), (23, 0.5230110497758668), (24, 0.5230110497758668)]\n",
      "[(10, 0.24434832234965204), (21, 0.33453001789363906), (22, 0.33453001789363906), (25, 0.4886966446993041), (26, 0.4886966446993041), (27, 0.4886966446993041)]\n",
      "[(1, 0.26450252657691997), (3, 0.19524002532943197), (7, 0.26450252657691997), (9, 0.36212253924163595), (28, 0.36212253924163595), (29, 0.5290050531538399), (30, 0.5290050531538399)]\n",
      "[(28, 0.36755247956451587), (31, 0.5369373566087501), (32, 0.5369373566087501), (33, 0.5369373566087501)]\n"
     ]
    }
   ],
   "source": [
    "for document in tfidf[corpus]:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating n-grams\n",
    "bigram = models.Phrases(texts) \n",
    "texts = [bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['football', 'club', 'Arsenal', 'defeat', 'local', 'rival', 'weekend'],\n",
       " ['weekend', 'football', 'frenzy', 'take', 'London'],\n",
       " ['bank', 'open', 'bid', 'lose', 'million'],\n",
       " ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'],\n",
       " ['arsenal', 'bid', 'pound', 'striker', 'Kane'],\n",
       " ['financial', 'trouble', 'result', 'loss', 'million', 'bank'],\n",
       " ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'],\n",
       " ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'],\n",
       " ['banking', 'finance', 'work', 'Russia']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arsenal': 0, 'club': 1, 'defeat': 2, 'football': 3, 'local': 4, 'rival': 5, 'weekend': 6, 'London': 7, 'frenzy': 8, 'take': 9, 'bank': 10, 'bid': 11, 'lose': 12, 'million': 13, 'open': 14, 'Wembley': 15, 'stadium': 16, 'Kane': 17, 'arsenal': 18, 'pound': 19, 'striker': 20, 'financial': 21, 'loss': 22, 'result': 23, 'trouble': 24, 'bankruptcy': 25, 'file': 26, 'western': 27, 'Russia': 28, 'millionaire': 29, 'oil': 30, 'banking': 31, 'finance': 32, 'work': 33}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<0 unique tokens: []>\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
