{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Introduction\n",
    "\n",
    "Household names like Echo (Alexa), Siri, and Google Translate have at least one thing in common. They are all products derived from the application of natural language processing (NLP).\n",
    "\n",
    "NLP refers to a set of techniques involving the application of statistical methods, with or without insights from linguistics, to understand text for the sake of solving real-world tasks. This “understanding” of text is mainly derived by transforming texts to useable computational representations, which are discrete or continuous combinatorial structures such as vectors or tensors, graphs, and trees.\n",
    "\n",
    "The learning of representations suitable for a task from data (text in this case) is the subject of machine learning. The application of machine learning to textual data has more than three decades of history, but in the last 10 years **`[1]`** a set of machine learning techniques known as deep learning have continued to evolve and begun to prove highly effective for various artificial intelligence (AI) tasks in NLP, speech, and computer vision. Deep learning is another main subject that we cover; thus, this book is a study of NLP and deep learning.\n",
    "\n",
    "**`[1]`** While the history of neural networks and NLP is long and rich, Collobert and Weston (2008) are often credited with pioneering the adoption of modern-style application deep learning to NLP.\n",
    "\n",
    "---\n",
    "\n",
    "NLP refers to a set of techniquies involving the application of statistical methods, with or without insights from linguistics, to understand text for the sake of solving real-world tasks. This \"understanding\" of text is mainly derived by transforming texts to useble computational representations, which are discrete or continuous combinatorial structures such as vectores or tensors, graphs and trees.\n",
    "\n",
    "The learning of representations suitable for a task from data (text in this case) is the subject of ML. The application of ML to textual data has more than three decades of history, bun in the last 10 years a set of ML techniques known as deep learning have continued to evolve and begun to prove highly effective for various AI tasks in NLP.\n",
    "\n",
    "Put simply, deep learning enables one to efficiently learn representations from data using an abstraction called the computational graph and numerical optimization techniques. Such is the success of deep learning and computational graphs that major tech companies such as Google, Facebook, and Amazon have published implementations of computational graph frameworks and libraries built on them to capture the mindshare of researchers and engineers.\n",
    "\n",
    "> supervised learning; that is, learning with labeled training examples\n",
    "\n",
    "## The Supervised Learning Paradigm\n",
    "Supervision in machine learning, or supervised learning, refers to cases where the ground truth for the targets (what’s being predicted) is available for the observations. For example, in document classification, the target is a categorical label,2 and the observation is a document. In machine translation, the observation is a sentence in one language and the target is a sentence in another language. With this understanding of the input data, we illustrate the supervised learning paradigm in Figure 1-1.\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 800px\">\n",
    "        <img src=\"imgs/nlpp_0101.png\">\n",
    "        <h4 style=\"font-family: courier; font-size: .8em;\">Figure 1-1. The supervised learning paradigm, a conceptual framework for learning from labeled input data.</h4>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "We can break down the supervised learning paradigm, as illustrated in Figure 1-1, to six main concepts:\n",
    "\n",
    "* **Observations**: Observations are items about which we want to predict something. We denote observations using $x$. We sometimes refer to the observations as inputs.\n",
    "* **Targets**: Targets are labels corresponding to an observation. These are usually the things being predicted. Following standard notations in machine learning/deep learning, we use y to refer to these. Sometimes, these labels known as the ground truth.\n",
    "* **Model**: A model is a mathematical expression or a function that takes an observation, x, and predicts the value of its target label.\n",
    "* **Parameters**: Sometimes also called weights, these parameterize the model. It is standard to use the notation w (for weights) or ŵ.\n",
    "* **Predictions**: Predictions, also called estimates, are the values of the targets guessed by the model, given the observations. We denote these using a “hat” notation. So, the prediction of a target y is denoted as $\\widehat{y}$.\n",
    "* **Loss function**: A loss function is a function that compares how far off a prediction is from its target for observations in the training data. Given a target and its prediction, the loss function assigns a scalar real value called the loss. The lower the value of the loss, the better the model is at predicting the target. We use $L$ to denote the loss function.\n",
    "\n",
    "---\n",
    "\n",
    "Although it is not strictly necessary to be mathematically formal to be productive in NLP/deep learning modeling or to write this book, we will formally restate the supervised learning paradigm to equip readers who are new to the area with the standard terminology so that they have some familiarity with the notations and style of writing in the research papers they may encounter on arXiv.\n",
    "\n",
    "Consider a dataset $\\large D= \\left\\{ X_{i}, y_{i}\\right\\} _{i=1}^{n}$ with n examples. Given this dataset, we want to learn a function (a model) f parameterized by weights w. That is, we make an assumption about the structure of f, and given that structure, the learned values of the weights w will fully characterize the model. For a given input X, the model predicts ŷ as the target:\n",
    "\n",
    "$$\\Large \\widehat{y} = f(X,w)$$\n",
    "\n",
    "In supervised learning, for training examples, we know the true target y for an observation. The loss for this instance will then be $\\large L(y, \\widehat{y})$. Supervised learning then becomes a process of finding the optimal parameters/weights w that will minimize the cumulative loss for all the n examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING USING (STOCHASTIC) GRADIENT DESCENT\n",
    "\n",
    "> The goal of supervised learning is to pick values of the parameters that minimize the loss function for a given dataset. In other words, this is equivalent to finding roots in an equation. We know that gradient descent is a common technique to find roots of an equation. Recall that in traditional gradient descent, we guess some initial values for the roots (parameters) and update the parameters iteratively until the objective function (loss function) evaluates to a value below an acceptable threshold (aka convergence criterion). For large datasets, implementation of traditional gradient descent over the entire dataset is usually impossible due to memory constraints, and very slow due to the computational expense. Instead, an approximation for gradient descent called stochastic gradient descent (SGD) is usually employed. In the stochastic case, a data point or a subset of data points are picked at random, and the gradient is computed for that subset. When a single data point is used, the approach is called pure SGD, and when a subset of (more than one) data points are used, we refer to it as minibatch SGD. Often the words “pure” and “minibatch” are dropped when the approach being used is clear based on the context. In practice, pure SGD is rarely used because it results in very slow convergence due to noisy updates. There are different variants of the general SGD algorithm, all aiming for faster convergence. In later chapters, we explore some of these variants along with how the gradients are used in updating the parameters. This process of iteratively updating the parameters is called backpropagation. Each step (aka epoch) of backpropagation consists of a forward pass and a backward pass. The forward pass evaluates the inputs with the current values of the parameters and computes the loss function. The backward pass updates the parameters using the gradient of the loss.\n",
    "\n",
    "Observe that until now, nothing here is specific to deep learning or neural networks.3 The directions of the arrows in Figure 1-1 indicate the “flow” of data while training the system. We will have more to say about training and on the concept of “flow” in “Computational Graphs”, but first, let’s take a look at how we can represent our inputs and targets in NLP problems numerically so that we can train models and predict outcomes.\n",
    "\n",
    "## Observation and Target Encoding\n",
    "We will need to represent the observations (text) numerically to use them in conjunction with machine learning algorithms. Figure 1-2 presents a visual depiction.\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 800px\">\n",
    "        <img src=\"imgs/nlpp_0102.png\">\n",
    "        <h4 style=\"font-family: courier; font-size: .8em;\">Figure 1-2. Observation and target encoding: The targets and observations from Figure 1-1 are represented numerically as vectors, or tensors. This is collectively known as input “encoding.”</h4>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "A simple way to represent text is as a numerical vector. There are innumerable ways to perform this mapping/representation. In fact, much of this book is dedicated to learning such representations for a task from data. However, we begin with some simple count-based representations that are based on heuristics. Though simple, they are incredibly powerful as they are and can serve as a starting point for richer representation learning. All of these count-based representations start with a vector of fixed dimension."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Representation\n",
    "\n",
    "The one-hot representation, as the name suggests, starts with a zero vector, and sets as 1 the corresponding entry in the vector if the word is present in the sentence or document. Consider the following two sentences:\n",
    "\n",
    "```\n",
    "Time flies like an arrow.\n",
    "Fruit flies like a banana.\n",
    "```\n",
    "\n",
    "Tokenizing the sentences, ignoring punctuation, and treating everything as lowercase, will yield a vocabulary of size 8: {time, fruit, flies, like, a, an, arrow, banana}. So, we can represent each word with an eight-dimensional one-hot vector. In this book, we use $\\large 1_w$ to mean one-hot representation for a token/word w.\n",
    "\n",
    "The collapsed one-hot representation for a phrase, sentence, or a document is simply a logical OR of the one-hot representations of its constituent words. Using the encoding shown in Figure 1-3, the one-hot representation for the phrase “like a banana” will be a 3×8 matrix, where the columns are the eight-dimensional one-hot vectors. It is also common to see a “collapsed” or a binary encoding where the text/phrase is represented by a vector the length of the vocabulary, with 0s and 1s to indicate absence or presence of a word. The binary encoding for “like a banana” would then be: `[0, 0, 0, 1, 1, 0, 0, 1]`.\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 600px\">\n",
    "        <img src=\"imgs/nlpp_0103.png\">\n",
    "        <h4 style=\"font-family: courier; font-size: .8em;\">Figure 1-3. One-hot representation for encoding the sentences \"Time flies like an arrow\" and \"Fruit flies like a banana.\"</h4>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "> NOTE: At this point, if you are cringing that we collapsed the two different meanings (or senses) of “flies,” congratulations, astute reader! Language is full of ambiguity, but we can still build useful solutions by making horribly simplifying assumptions. It is possible to learn sense-specific representations, but we are getting ahead of ourselves now.\n",
    "\n",
    "Although we will rarely use anything other than a one-hot representation for the inputs in this book, we will now introduce the Term-Frequency (TF) and Term-Frequency-Inverse-Document-Frequency (TF-IDF) representations. This is done because of their popularity in NLP, for historical reasons, and for the sake of completeness. These representations have a long history in information retrieval (IR) and are actively used even today in production NLP systems.\n",
    "\n",
    "## TF (Term-Frequency) Representation\n",
    "\n",
    "The TF representation of a phrase, sentence, or document is simply the sum of the one-hot representations of its constituent words. To continue with our silly examples, using the aforementioned one-hot encoding, the sentence “Fruit flies like time flies a fruit” has the following TF representation: `[1, 2, 2, 1, 1, 0, 0, 0]`. Notice that each entry is a count of the number of times the corresponding word appears in the sentence (corpus). We denote the TF of a word w by TF(w)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = ['Time flies flies like an arrow.',\n",
    "          'Fruit flies like a banana.']\n",
    "\n",
    "one_hot_vectorizer = CountVectorizer(binary=True)\n",
    "one_hot = one_hot_vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "vocab = one_hot_vectorizer.vocabulary_\n",
    "vocab = dict(sorted(vocab.items(), key=lambda v: v[1]))\n",
    "vocab = list(vocab.keys())\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.heatmap(one_hot, annot=True, cbar=False, xticklabels=vocab, yticklabels=['Sentence 1','Sentence 2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Representation\n",
    "\n",
    "Consider a collection of patent documents. You would expect most of them to contain words like claim, system, method, procedure, and so on, often repeated multiple times. The TF representation weights words proportionally to their frequency. However, common words such as “claim” do not add anything to our understanding of a specific patent. Conversely, if a rare word (such as “tetrafluoroethylene”) occurs less frequently but is quite likely to be indicative of the nature of the patent document, we would want to give it a larger weight in our representation. The Inverse-Document-Frequency (IDF) is a heuristic to do exactly that.\n",
    "\n",
    "The IDF representation penalizes common tokens and rewards rare tokens in the vector representation. The IDF(w) of a token w is defined with respect to a corpus as:\n",
    "\n",
    "$$\\Large IDF(w) = \\log \\dfrac{N}{n_w}$$\n",
    "\n",
    "where $\\large n_w$ is the number of documents containing the word w and N is the total number of documents. The TF-IDF score is simply the product TF(w) * IDF(w). First, notice how if there is a very common word that occurs in all documents (i.e., $n_w = N$), IDF(w) is 0 and the TF-IDF score is 0, thereby completely penalizing that term. Second, if a term occurs very rarely, perhaps in only one document, the IDF will be the maximum possible value, log N. Example 1-2 shows how to generate a TF-IDF representation of a list of English sentences using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr0AAAD4CAYAAADhLl4uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAd3klEQVR4nO3deZRcVbn38e9T3Z0BEDKREZIwIyAECAEUEESZlEHUsMAZFNBXRsWlgsIV8UW5oCIoolzBAV5AuAgIIYjMEKaQQJhlEAIhkJAOIWToYb9/VCXpJN2dEutUdZ/6ftaq1XVO7a56evfuOr/etetUpJSQJEmS8qxQ6wIkSZKkrBl6JUmSlHuGXkmSJOWeoVeSJEm5Z+iVJElS7jVm/QDvnHyQp4dQXRpwwSO1LqGuLXrt7lqXULf6j9y91iXUtSlDd6p1CXVty4nttS6hrq1z3vXR1W3O9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKvcZaF1ArDVvuQN9DvgKFBlqmTKblH9d02q6w4ab0P+EcFv/hHNoeuw8am+j/jf8LjU1QaKBt+r0sveWKKlff+9n/Pdu+++zJeef9kIZCgf/5/RX89JwLa11Sbtwz5WHO/vlFtLW386kD9+Mrn5+4WpsHpz7GT37xG1pbWxk4YF0uvfAcAE778Xncde+DDBo4gOv+dFG1S68Ljv3qWXfP7Rn9X1+BhgJzrriV1y+8dqXbB+wzgZGnHAHtidTaxitnXMI7Dz1Vo2p7P4+79Rp6o0DfQ49h0UU/IM2fS/+TzqX1iQdJs19ZrV2fT3yJtmceXbGvtYVFvzoNli6GQgP9jzubwtNTaf/XM9X9GXoz+79HKxQKnP+Ls9jvgMOZOXMWU+6/iRtunMxTTz1X69J6vba2Nn507oX89uc/ZvjQIRz2lRPYa7ed2WSjMcvbvL3gHX507gX85twfMWL4UObOa15+2yEHfIwjPnUQ3zvzv2tQff459quoUGD0j47h2SNOp2XWXN7/t3Nonvwgi5+bubzJ2/c8RvPkBwHo//4xbPzrU3hiz2/UquLezeMu8B6XN0TExypdSDUVRm9G+5xZpLdmQ1srrY/eTeM2O6/Wrmn3T9D22H2kBfNXvmHp4uLXhgZoaISUqlB1ftj/PduEnbbn+edf4sUXX6alpYWrrvorBx24b63LyoXHn3qW0RuMZMNRI2hqamL/vT/MP+6eslKbm269g49++EOMGD4UgMEDByy/bfy4D7Deuu+rZsl1xbFfPWuP24wlL81i6cuzSS2tvPXXexiwz8rHgfZ3Fy+/Xujfz+f6/4DH3aL3uqb3kopWUWWx3mBS85zl26l5DrHe4FXaDKLxA7vQct+kTu6gQP9v/py1f/hH2p6dRvvLz2Zdcq7Y/z3byFHDeWXma8u3Z746i5Ejh9ewovx44805DB+6/vLtYUOH8Mabc1dq89LLM3l7wTt86RvfZuKRx/HXm/9e7TLrlmO/evqMGMTSWSuOA0tfn0ufEYNWazdgv53Z+o4L2OwPp/HSNy+oZom54nG3qMvlDRFxfVc3AYO7uG3Z9x4NHA3wi7235chtx3TXvPoiVt+3yn8tfQ/+KktuvAxSeydt21l07onQb236HfldCsNH0/76y9nUmkf2f48Wnfx+Ui/9r76n6awbV+3utrZ2nnz6OX53/tksWbKEzx5zMtttvSVjR29QnSLrmGO/mjrr69VbNU96gOZJD7DOzlsx6pQjePbw06tQWw553AW6X9O7O/A54J1V9gcwobs7TSldDFwM8M7JB/W4Z4zUPIcYMGT5dgwYQnr7rZXaFDbclH6f/1bx9rXXpeH9O7KkvY22GQ+saLR4IW3/nEHDljv0yl9+rdj/PdurM2ex4QYjl29vMGoEs2bNrmFF+TFs6BBef+PN5duz35jD+kMGr9ZmwIB1Wat/P9bq348dx23DM/980dBbBY796lk6ay59Rqw4DvQZPpiW19/qsv07DzxJ3zHDaRz4PlrnLahGibnicbeou+UNU4B3U0p3rnK5A+h9q5c7aH/lOQrrjyQGDYOGRhq3333lXyrw7llf5d0fFS+t0+9jyTUXFdusvS70W7vYqKkPjZtvR/sbMzt5FHXF/u/ZHnp4GptuuhFjx25IU1MTEycezA03Tq51WbmwzZab8/LM15j52uu0tLRw8213stduu6zUZq/dd2Hq9Bm0traxaPFiHn/iGTYeu2GNKq4vjv3qWTj9OfptNII+Gw4lmhoZdPBuNN/64Ept+o5dsbRkrW02Jvo0GnjfI4+7RV3O9KaU9u/mtj2yKadK2ttZcu1v6H/0GVAo0PLg32mf/QqNu+4HQOv9naxnKSmsO4i+h58IhQJE0Dr9HtqefLg6deeF/d+jtbW1ccKJp3HT3y6noVDg0suu5Mkne+f6rZ6msbGB7530NY45+TTa2tr45Cf2YdONx3Dl//4NgMM++XE2GTuaD+08nkO/+DUKUeBTB+7LZhuPBeCU08/moUcfo7n5bfY+5HN8/ajP8ynfaFUxjv0qamvn5e//ls3/fDoUGph75d9Z/OwrrP+54nh+80+3MPCAXRn8qb1IrW20L17CC1/zrCXvmcddACLr9Uo9cXmDVA0DLnik1iXUtUWv3V3rEupW/5G717qEujZl6E61LqGubTmxkzWxqpp1zru+kwXMRX4imyRJknLP0CtJkqTcKyv0RkT/iNgi62IkSZKkLKwx9EbEgcA0YFJpe1w35/CVJEmSepxyZnrPoHhe3maAlNI0YGxWBUmSJEmVVk7obU0pzV9zM0mSJKln6u4T2ZaZERFHAA0RsRlwPHBftmVJkiRJlVPOTO9xwNbAEuByYD5wYoY1SZIkSRW1xpnelNK7wKmliyRJktTrlHP2hlsjYkCH7YERcUumVUmSJEkVVM7yhiEppeZlGymlecDQzCqSJEmSKqyc0NseEaOXbUTEGCBlV5IkSZJUWeWcveFU4J6IuLO0vQdwdHYlSZIkSZVVzhvZJkXEDsAuQAAnpZTmZF6ZJEmSVCHlzPQC9AXeKrXfKiJIKd2VXVmSJElS5awx9EbET4DDgCeA9tLuBBh6JUmS1CuUM9N7CLBFSmlJxrVIkiRJmSjn7A0vAE1ZFyJJkiRlpZyZ3neBaRFxG8WPIgYgpXR8ZlVJkiRJFVRO6L2+dJEkSZJ6pXJOWXZZRPQHRqeUnqlCTZIkSVJFrXFNb0QcCEwDJpW2x0WEM7+SJEnqNcp5I9sZwASgGSClNA3YKLOKJEmSpAorJ/S2ppTmr7IvZVGMJEmSlIVy3sg2IyKOABoiYjPgeOC+bMuSJEmSKqecmd7jgK0pnq7scmA+cEKWRUmSJEmVVM5M78dTSqcCpy7bERGfAa7OrCpJkiSpgsqZ6f1umfskSZKkHqnLmd6I2B84ABgVEed3uGldoDXrwiRJkqRK6W55w2vAw8BBwCMd9i8ATsqyKEmSJKmSugy9KaXpwPSIuDyl1FLFmiRJkqSKKueNbBMi4gxgTKl9ACmltHGWhUmSJEmVUk7ovYTicoZHgLZsy5EkSZIqr5zQOz+ldHPmlUiSJEkZKSf03h4R5wDXUvyACgBSSlMzq0qSJEmqoHJC786lr+M77EvARypfjiRJklR5awy9KaW9qlGIJEmSlJU1fiJbRAyLiEsi4ubS9lYRcVT2pUmSJEmVUc7HEF8K3AKMLG0/C5yYUT2SJElSxZUTeoeklK4C2gFSSq146jJJkiT1IuWE3oURMZjim9eIiF2A+ZlWJUmSJFVQOWdvOBm4HtgkIu4F1gc+nWlVkiRJUgWVc/aGqRHxYWALih9B/ExKqSXzyiRJkqQK6XJ5Q0TsFBHDYfk63h2Bs4BzI2JQleqTJEmS/mPdren9DbAUICL2AM4G/kBxPe/F2ZcmSZIkVUZ3yxsaUkpvla4fBlycUroGuCYipmVemSRJklQh3c30NkTEslC8N/CPDreV8wY4SZIkqUfoLrxeAdwZEXOARcDdABGxKZ6yTJIkSb1Il6E3pXRWRNwGjAAmp5RS6aYCcFw1ipMkSZIqIVZk2Ww09hmV7QNIPdTCGVfWuoS6tuTcM2tdQt3q+83v17qEuubYr61Bv59R6xLqWuvSV6Or28r5RDZJkiSpVzP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3DP0SpIkKfcMvZIkSco9Q68kSZJyz9ArSZKk3GusdQE91b777Ml55/2QhkKB//n9Ffz0nAtrXVJdsf+zdc8jj/OT315Be3vi0I/tzlGfOWCl2x96/GlO+NEFjBo2BIC9d92BYw8/CIA/XjeZayffDQGbjd2AM084kr59mqr+M/RWDVuPp9/EY4lCA0vvuZmlt1zVabvCmM1Z+zs/Z9Fvf0zr1HsA6PeFk2n8wM6kBc0s/OEx1Sw7Nxz7teX477nq4bhr6O1EoVDg/F+cxX4HHM7MmbOYcv9N3HDjZJ566rlal1YX7P9stbW18+OL/szFZ36TYYMHcvjJZ7LnzuPYZPTIldrtsNVmXHD6CSvtmz13Hn++4Tau+9WZ9Ovbh2+d/Wsm3fUAB390t2r+CL1XFOh/+P9h4c+/S5o3h7W/+0taH5tC+6yXV2vX79CjaH3ikZV2t9w/maW3X0//L59SxaLzw7FfY47/Hqtejrsub+jEhJ225/nnX+LFF1+mpaWFq676KwcduG+ty6ob9n+2Zjz3AqNHDGWD4evT1NTIfntM4PYHHi37+9va21iydCmtbW0sXrKU9QcNyK7YnGnYaAva33iNNOd1aGul5eE7aNxu19Xa9fnIwbQ8eg9pQfNK+9uem0F6d0GVqs0fx35tOf57rno57nYbeiNi3YjYpJP922ZXUu2NHDWcV2a+tnx75quzGDlyeA0rqi/2f7Zmz21m2JBBy7eHDR7IG3ObV2s3/Znn+fRxp/O103/GP//16vK2X/zkvuxz5LfZ+wsns87a/fngDttUq/ReLwYMpn3em8u307w5FAYMWa1N47gP0nLn36pdXu459mvL8d9z1ctxt8vQGxETgaeBayLiiYjYqcPNl3Z3pxFxdEQ8HBEPt7cvrEylVRQRq+1LKdWgkvpk/2esk75ctcvfv8kYbrnkp/zll//FEQfuzYlnXQDA2+8s5PYHpnHz737C3y87l0WLl3Dj7fdXo+qcWH1sw8q/j34Tj2XJtZdAaq9OSfXEsV9jjv+eql6Ou93N9H4P2DGlNA74MvDHiDi0dFtnI3e5lNLFKaXxKaXxhcLalam0il6dOYsNN1ixxmuDUSOYNWt2DSuqL/Z/toYNGcjsOW8t3549d95qL9Ous1Z/1urfD4Ddx29La1sb8+YvYMq0J9lg2BAGrfc+mhob2fuDOzLtqX9Ws/xeLTXPoTBw/eXbMXAI7c1zV2rTMGZz+n/lu6xz1mU07bA7/Q4/rtOXgPXvc+zXluO/56qX4253obchpTQLIKX0ILAXcGpEHM+q/5rlzEMPT2PTTTdi7NgNaWpqYuLEg7nhxsm1Lqtu2P/Z2nqzjfjXa7OZ+fqbtLS0MumuB9lzwriV2syZN3/5f/mPP/sC7e2JAeuuw/D1B/PY0y+waPESUko8MP0pNt5wZCePos60vfQMhaGjiMHDoKGRpvF70jp9ykpt3jn1i8svLVPvZvEVv6R1ujOKleDYry3Hf89VL8fd7s7esCAiNkkpPQ+QUpoVEXsC1wFbZ19a7bS1tXHCiadx098up6FQ4NLLruTJJ5+tdVl1w/7PVmNDA9879rN87fSf0dbeziEf3Y1Nx4ziqpvvAGDi/nty670Pc9VNd9DQUKBv3z789NvHEBFsu8XGfPRDO3LYiT+koaHA+zcezaf326O2P1Bv0t7O4v93IWud8GOiUGDpvZNpn/Uvmvb4OAAtd3W/jrH/Ud+hYYttiXXWY52z/8SSG/5Iy723VKPyXHDs15jjv8eql+NudLVmIyK2AxamlP65yv4mYGJK6c/lPEBjn1G5nhWWurJwxpW1LqGuLTn3zFqXULf6fvP7tS6hrjn2a2vQ72fUuoS61rr01S6X4HY505tSmt7F/hagrMArSZIk9QSep1eSJEm5Z+iVJElS7pUVeiOif0RskXUxkiRJUhbWGHoj4kBgGjCptD0uIq7PuC5JkiSpYsqZ6T0DmAA0A6SUpgFjsypIkiRJqrRyQm9rSml+5pVIkiRJGenuwymWmRERRwANEbEZcDxwX7ZlSZIkSZVTzkzvcRQ/gW0JcDkwHzgxw5okSZKkilrjTG9K6V3g1NJFkiRJ6nXKOXvDrRExoMP2wIjww64lSZLUa5SzvGFISql52UZKaR4wNLOKJEmSpAorJ/S2R8ToZRsRMQZI2ZUkSZIkVVY5Z284FbgnIu4sbe8BHJ1dSZIkSVJllfNGtkkRsQOwCxDASSmlOZlXJkmSJFVIOTO9AH2Bt0rtt4oIUkp3ZVeWJEmSVDlrDL0R8RPgMOAJoL20OwGGXkmSJPUK5cz0HgJskVJaknEtkiRJUibKOXvDC0BT1oVIkiRJWSlnpvddYFpE3Ebxo4gBSCkdn1lVkiRJUgWVE3qvL10kSZKkXqmcU5ZdFhH9gdEppWeqUJMkSZJUUWtc0xsRBwLTgEml7XER4cyvJEmSeo1y3sh2BjABaAZIKU0DNsqsIkmSJKnCygm9rSml+avsS1kUI0mSJGWhnDeyzYiII4CGiNgMOB64L9uyJEmSpMopZ6b3OGBriqcruxyYD5yQZVGSJElSJZUz0/vxlNKpwKnLdkTEZ4CrM6tKkiRJqqByZnq/W+Y+SZIkqUfqcqY3IvYHDgBGRcT5HW5aF2jNujBJkiSpUrpb3vAa8DBwEPBIh/0LgJOyLEqSJEmqpC5Db0ppOjA9Ii5PKbVUsSZJkiSposp5I9uEiDgDGFNqH0BKKW2cZWGSJElSpZQTei+huJzhEaAt23IkSZKkyisn9M5PKd2ceSWSJElSRsoJvbdHxDnAtRQ/oAKAlNLUzKqSJEmSKqic0Ltz6ev4DvsS8JHKlyNJkiRV3hpDb0ppr2oUIkmSJGVljZ/IFhHDIuKSiLi5tL1VRByVfWmSJElSZZTzMcSXArcAI0vbzwInZlSPJEmSVHHlhN4hKaWrgHaAlFIrnrpMkiRJvUg5oXdhRAym+OY1ImIXYH6mVUmSJEkVVM7ZG04Grgc2iYh7gfWBT2dalSRJklRB5Zy9YWpEfBjYguJHED+TUmrJvDJJkiSpQrpc3hARO0XEcFi+jndH4Czg3IgYVKX6JEmSpP9Yd2t6fwMsBYiIPYCzgT9QXM97cfalSZIkSZXR3fKGhpTSW6XrhwEXp5SuAa6JiGmZVyZJkiRVSHczvQ0RsSwU7w38o8Nt5bwBTpIkSeoRuguvVwB3RsQcYBFwN0BEbIqnLJMkSVIv0mXoTSmdFRG3ASOAySmlVLqpABxXjeIkSZKkSogVWVadiYijU0q+ca9G7P/ase9ry/6vLfu/duz72spz/5fziWz17uhaF1Dn7P/ase9ry/6vLfu/duz72spt/xt6JUmSlHuGXkmSJOWeoXfNcrmupRex/2vHvq8t+7+27P/ase9rK7f97xvZJEmSlHvO9EqSJCn3DL2SJEnKPUOvlEMRMTYiZtS6DnUuIo6PiKci4tWIuKC079iI+EKta8u7Dn3/53/je26KiAGly9ezrK9eRMQ7pa8jI+IvpetfWvb3oMrqOHY79nm9cU2veoyIaEgptXW1rfJFxFjgxpTSNrWuRauLiKeB/YEPA+NTSt+ocUl1Y1nfp5Re7LCvMaXUWsb3jsW/q4qIiHdSSuussu9L+PeQCcdukTO9HUTEdRHxSEQ8ERFHl/a9ExFnRcT0iJgSEcNqXWdv1U3//jAiHgB27WT75IiYUbqcWPqeb0fE8aXrP4uIf5Su7x0Rf6rRj9cTNUbEZRHxWET8JSLWiogfRMRDpf68OCICICLuiIifRMSDEfFsROxe2j82Iu6OiKmlywdL+/csfc9fIuLpiPhzh/vq9DFUFBEXARsD1wMDO+w/IyK+Vbq+SURMKv293B0RW5b2f6bUr9Mj4q6a/AC9WMe+j4j5pfE5GfjDqrOMEXFjROxZuv5SRAwBzgY2iYhpEXFODX6E3OnqVamI+HhE3B8RQyJin9L1qRFxdUSs09l9qVsdx+7Vy/q8NO6vi4gbIuLFiPhG6bj7aCnzDCq16/Q5qbcx9K7syJTSjsB44PiIGAysDUxJKW0H3AV8tZYF9nJd9e+MlNLOKaV7Om4Di4AvAzsDuwBfjYjtKf4edi/d53hgnYhoAnYD7q7qT9SzbQFcnFLaFngb+DpwQUppp9J/+/2BT3Ro35hSmgCcCJxe2vcG8LGU0g7AYcD5HdpvX2q7FcUg8aHS/u4eo+6llI4FXgP2AuZ10exi4LjS38u3gF+V9v8A2Lf0fHRQ1rXmzSp9/zNgR+DglNIRZd7Fd4DnU0rjUkqnZFRm3YuIT1Ls6wNKu04DPlp6HnoYOLlWtfViy8cusOrY3QY4ApgAnAW8m1LaHrgfWLbkqqvnpF6lsdYF9DDHl/7YADYENgOWAjeW9j0CfKwWheVEZ/3bBlzToU3H7d2A/00pLQSIiGspht1fAztGxPuAJcBUiuF3d+D4rH+IXuSVlNK9pet/otg3L0bEt4G1gEHAE8ANpTbXlr4+AowtXW8CLoiIcRR/N5t3uP8HU0ozASJiWul77gH26uYxtAalWawPAld3mCTvW/p6L3BpRFzFit+X3rvrU0qLal2EVrIXxefzfVJKb0fEJyj+Y31v6e+hD8Uwpsq5PaW0AFgQEfNZ8Xz9OLDtGp6TehVDb0npZayPArumlN6NiDuAfkBLWrHwuQ377D3ppn8Xr7Jut+N2py+Lp5RaIuIlirPA9wGPUXyi3AR4Kov6e6lVF+wniv+dj08pvRIRZ1D8HSyzpPS14zg/CZgNbEfxlaHFnbRf/j0R0W8Nj6E1KwDNpRmZlaSUjo2InYGPA9MiYlxKaW61C8yRhR2ut7Lyq5+O29p4geIrR5tTnNUN4NaU0uE1rSrfOj6Xt3fYbqd4LOjyOam3cXnDCusB80qBbEuKL6erct5L/94FHFJai7o28ElWLF+4i+JLLHeV9h0LTOvwD4pgdETsWrp+OMVZWIA5pf/cP13GfawHzEoptQOfBxrW0H5ZUPh3HkMdpJTepjgj/xmAKNqudH2TlNIDKaUfAHMovmKiyngJGBcRhYjYkOJLvataALyvqlXVn38Bh1JcZ701MAX4UERsClA6Hmze3R2oU+957Hb3nNTbGHpXmERxpuox4EyKf2iqnH+7f1NKU4FLgQeBB4DfpZQeLd18NzACuD+lNJviDKTreVf2FPDFUp8Porgs5LcUX7K6DniojPv4Vek+plCceVnYXeOUUvN7eAyt7rPAURExneLykINL+8+JiMdLb0K5C5heqwJz6F7gRYpj978pLptaSWlW/d7Smwl9I1tGUkrPUPwbuBpYF/gScEXpuWwK0CvfRFVLHccu8F7GblfPSb2KpyyTJElS7jnTK0mSpNwz9EqSJCn3DL2SJEnKPUOvJEmScs/QK0mSpNwz9EqSJCn3DL2SJEnKvf8PE9rvyq0K8rgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.heatmap(tfidf, annot=True, cbar=False, xticklabels=vocab, yticklabels= ['Sentence 1', 'Sentence 2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, it is rare to see inputs encoded using heuristic representations like TF-IDF because the goal is to learn a representation. Often, we start with a one-hot encoding using integer indices and a special “embedding lookup” layer to construct inputs to the neural network. In later chapters, we present several examples of doing this.\n",
    "\n",
    "## Target Encoding\n",
    "\n",
    "As noted in the “The Supervised Learning Paradigm”, the exact nature of the target variable can depend on the NLP task being solved. For example, in cases of machine translation, summarization, and question answering, the target is also text and is encoded using approaches such as the previously described one-hot encoding.\n",
    "\n",
    "Many NLP tasks actually use categorical labels, wherein the model must predict one of a fixed set of labels. A common way to encode this is to use a unique index per label, but this simple representation can become problematic when the number of output labels is simply too large. An example of this is the language modeling problem, in which the task is to predict the next word, given the words seen in the past. The label space is the entire vocabulary of a language, which can easily grow to several hundred thousand, including special characters, names, and so on. We revisit this problem in later chapters and see how to address it.\n",
    "\n",
    "Some NLP problems involve predicting a numerical value from a given text. For example, given an English essay, we might need to assign a numeric grade or a readability score. Given a restaurant review snippet, we might need to predict a numerical star rating up to the first decimal. Given a user’s tweets, we might be required to predict the user’s age group. Several approaches exist to encode numerical targets, but simply placing the targets into categorical “bins”—for example, “0-18,” “19-25,” “25-30,” and so on—and treating it as an ordinal classification problem is a reasonable approach.4 The binning can be uniform or nonuniform and data-driven. Although a detailed discussion of this is beyond the scope of this book, we draw your attention to these issues because target encoding affects performance dramatically in such cases, and we encourage you to see Dougherty et al. (1995) and the references therein."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graphs\n",
    "\n",
    "[Figure 1-1] summarized the supervised learning (training) paradigm as a data flow architecture where the inputs are transformed by the model (a mathematical expression) to obtain predictions, and the loss function (another expression) to provide a feedback signal to adjust the parameters of the model. This data flow can be conveniently implemented using the computational graph data structure[5]. Technically, a computational graph is an abstraction that models mathematical expressions. In the context of deep learning, the implementations of the computational graph (such as Theano, TensorFlow, and PyTorch) do additional bookkeeping to implement automatic differentiation needed to obtain gradients of parameters during training in the supervised learning paradigm. We explore this further in [“PyTorch Basics”]. *Inference* (or prediction) is simply expression evaluation (a forward flow on a computational graph). Let’s see how the computational graph models expressions. Consider the expression:\n",
    "\n",
    "$$\\Large y=wx+b$$\n",
    "\n",
    "This can be written as two subexpressions, $z = wx$ and $y = z + b$. We can then represent the original expression using a directed acyclic graph (DAG) in which the nodes are the mathematical operations, like multiplication and addition. The inputs to the operations are the incoming edges to the nodes and the output of each operation is the outgoing edge. So, for the expression $y = wx + b$, the computational graph is as illustrated in [Figure 1-6]. In the following section, we see how PyTorch allows us to create computational graphs in a straightforward manner and how it enables us to calculate the gradients without concerning ourselves with any bookkeeping.\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 600px\">\n",
    "        <img src=\"imgs/nlpp_0105.png\">\n",
    "        <h4 style=\"font-family: courier; font-size: .8em;\">Figure 1-6. Representing y = wx + b using a computational graph.</h4>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# References\n",
    "\n",
    "* Dougherty, James, Ron Kohavi, and Mehran Sahami. (1995). “Supervised and Unsupervised Discretization of Continuous Features.” Proceedings of the 12th International Conference on Machine Learning.\n",
    "* Collobert, Ronan, and Jason Weston. (2008). “A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.” Proceedings of the 25th International Conference on Machine Learning.\n",
    "\n",
    "---\n",
    "\n",
    "# Notes\n",
    "\n",
    "1. While the history of neural networks and NLP is long and rich, Collobert and Weston (2008) are often credited with pioneering the adoption of modern-style application deep learning to NLP.\n",
    "2. A categorical variable is one that takes one of a fixed set of values; for example, {TRUE, FALSE}, {VERB, NOUN, ADJECTIVE, ...}, and so on.\n",
    "3. Deep learning is distinguished from traditional neural networks as discussed in the literature before 2006 in that it refers to a growing collection of techniques that enabled reliability by adding more layers in the network. We study why this is important in Chapters 3 and 4.\n",
    "4. An “ordinal” classification is a multiclass classification problem in which there exists a partial order between the labels. In our age example, the category “0–18” comes before “19–25,” and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. A Quick Tour of Traditional NLP\n",
    "\n",
    "Natural language processing (NLP, introduced in the previous chapter) and computational linguistics (CL) are two areas of computational study of human language. NLP aims to develop methods for solving practical problems involving language, such as information extraction, automatic speech recognition, machine translation, sentiment analysis, question answering, and summarization. CL, on the other hand, employs computational methods to understand properties of human language. How do we understand language? How do we produce language? How do we learn languages? What relationships do languages have with one another?\n",
    "\n",
    "In literature, it is common to see a crossover of methods and researchers, from CL to NLP and vice versa. Lessons from CL about language can be used to inform priors in NLP, and statistical and machine learning methods from NLP can be applied to answer questions CL seeks to answer. In fact, some of these questions have ballooned into disciplines of their own, like phonology, morphology, syntax, semantics, and pragmatics.\n",
    "\n",
    "In this book, we concern ourselves with only NLP, but we borrow ideas routinely from CL as needed. Before we fully vest ourselves into neural network methods for NLP—the focus of the rest of this book—it is worthwhile to review some traditional NLP concepts and methods. That is the goal of this chapter.\n",
    "\n",
    "If you have some background in NLP, you can skip this chapter, but you might as well stick around for nostalgia and to establish a shared vocabulary for the future.\n",
    "\n",
    "## Corpora, Tokens, and Types\n",
    "All NLP methods, be they classic or modern, begin with a text dataset, also called a corpus (plural: corpora). A corpus usually contains raw text (in ASCII or UTF-8) and any metadata associated with the text. The raw text is a sequence of characters (bytes), but most times it is useful to group those characters into contiguous units called tokens. In English, tokens correspond to words and numeric sequences separated by white-space characters or punctuation.\n",
    "\n",
    "The metadata could be any auxiliary piece of information associated with the text, like identifiers, labels, and timestamps. In machine learning parlance, the text along with its metadata is called an instance or data point. The corpus (Figure 2-1), a collection of instances, is also known as a dataset. Given the heavy machine learning focus of this book, we freely interchange the terms corpus and dataset throughout.\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 600px\">\n",
    "        <img src=\"imgs/nlpp_0201.png\">\n",
    "        <h4 style=\"font-family: courier; font-size: .8em;\">Figure 2-1. The corpus: the starting point of NLP tasks.</h4>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "The process of breaking a text down into tokens is called tokenization. For example, there are six tokens in the Esperanto sentence “Maria frapis la verda sorĉistino.”1 Tokenization can become more complicated than simply splitting text based on nonalphanumeric characters, as is demonstrated in Figure 2-2. For agglutinative languages like Turkish, splitting on whitespace and punctuation might not be sufficient, and more specialized techniques might be warranted. As you will see in Chapters 4 and 6, it may be possible to entirely circumvent the issue of tokenization in some neural network models by representing text as a stream of bytes; this becomes very important for agglutinative languages.\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 600px\">\n",
    "        <img src=\"imgs/nlpp_0202.png\">\n",
    "        <h4 style=\"font-family: courier; font-size: .8em;\">Figure 2-2. Tokenization in languages like Turkish can become complicated quickly.</h4>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Finally, consider the following tweet:\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 600px\">\n",
    "        <img src=\"imgs/nlpp_02_jesustweet.png\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Tokenizing tweets involves preserving hashtags and @handles, and segmenting smilies such as :-) and URLs as one unit. Should the hashtag #MakeAMovieCold be one token or four? Most research papers don’t give much attention to these matters, and in fact, many of the tokenization decisions tend to be arbitrary—but those decisions can significantly affect accuracy in practice more than is acknowledged. Often considered the grunt work of preprocessing, most open source NLP packages provide reasonable support for tokenization to get you started. Example 2-1 shows examples from NLTK and spaCy, two commonly used packages for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', 'n’t', 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"Mary, don’t slap the green witch\"\n",
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet=u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:-)\"\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types are unique tokens present in a corpus. The set of all types in a corpus is its vocabulary or lexicon. Words can be distinguished as content words and stopwords. Stopwords such as articles and prepositions serve mostly a grammatical purpose, like filler holding the content words.\n",
    "\n",
    "> FEATURE ENGINEERING\n",
    "\n",
    "> This process of understanding the linguistics of a language and applying it to solving NLP problems is called feature engineering. This is something that we keep to a minimum here, for convenience and portability of models across languages. But when building and deploying real-world production systems, feature engineering is indispensable, despite recent claims to the contrary. For an introduction to feature engineering in general, consider reading the book by Zheng and Casari (2016).\n",
    "\n",
    "## Unigrams, Bigrams, Trigrams, …, N-grams\n",
    "N-grams are fixed-length (n) consecutive token sequences occurring in the text. A bigram has two tokens, a unigram one. Generating n-grams from a text is straightforward enough, as illustrated in Example 2-2, but packages like spaCy and NLTK provide convenient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mary', ',', \"n't\"],\n",
       " [',', \"n't\", 'slap'],\n",
       " [\"n't\", 'slap', 'green'],\n",
       " ['slap', 'green', 'witch'],\n",
       " ['green', 'witch', '.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def n_grams(text, n):\n",
    "    '''\n",
    "    takes tokens or text, returns a list of n-grams\n",
    "    '''\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "cleaned = ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
    "\n",
    "n_grams(cleaned, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some situations in which the subword information itself carries useful information, one might want to generate character n-grams. For example, the suffix “-ol” in “methanol” indicates it is a kind of alcohol; if your task involved classifying organic compound names, you can see how the subword information captured by n-grams can be useful. In such cases, you can reuse the same code, but treat every character n-gram as a token.2\n",
    "\n",
    "## Lemmas and Stems\n",
    "Lemmas are root forms of words. Consider the verb fly. It can be inflected into many different words—flow, flew, flies, flown, flowing, and so on—and fly is the lemma for all of these seemingly different words. Sometimes, it might be useful to reduce the tokens to their lemmas to keep the dimensionality of the vector representation low. This reduction is called lemmatization, and you can see it in action in Example 2-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he --> he\n",
      "was --> be\n",
      "running --> run\n",
      "late --> late\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"he was running late\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token} --> {token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy, for example, uses a predefined dictionary, called WordNet, for extracting lemmas, but lemmatization can be framed as a machine learning problem requiring an understanding of the morphology of the language.\n",
    "\n",
    "Stemming is the poor-man’s lemmatization.3 It involves the use of handcrafted rules to strip endings of words to reduce them to a common form called stems. Popular stemmers often implemented in open source packages include the Porter and Snowball stemmers. We leave it to you to find the right spaCy/NLTK APIs to perform stemming.\n",
    "\n",
    "## Categorizing Sentences and Documents\n",
    "\n",
    "Categorizing or classifying documents is probably one of the earliest applications of NLP. The TF and TF-IDF representations we described in Chapter 1 are immediately useful for classifying and categorizing longer chunks of text such as documents or sentences. Problems such as assigning topic labels, predicting sentiment of reviews, filtering spam emails, language identification, and email triaging can be framed as supervised document classification problems. (Semi-supervised versions, in which only a small labeled dataset is used, are incredibly useful, but that topic is beyond the scope of this book.)\n",
    "\n",
    "## Categorizing Words: POS Tagging\n",
    "\n",
    "We can extend the concept of labeling from documents to individual words or tokens. A common example of categorizing words is part-of-speech (POS) tagging, as demonstrated in Example 2-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary - PROPN\n",
      "slapped - VERB\n",
      "the - DET\n",
      "green - ADJ\n",
      "witch - NOUN\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token} - {token.pos_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing Spans: Chunking and Named Entity Recognition\n",
    "Often, we need to label a span of text; that is, a contiguous multitoken boundary. For example, consider the sentence, “Mary slapped the green witch.” We might want to identify the noun phrases (NP) and verb phrases (VP) in it, as shown here:\n",
    "\n",
    "> [NP Mary] [VP slapped] [the green witch].\n",
    "\n",
    "This is called chunking or shallow parsing. Shallow parsing aims to derive higher-order units composed of the grammatical atoms, like nouns, verbs, adjectives, and so on. It is possible to write regular expressions over the part-of-speech tags to approximate shallow parsing if you do not have data to train models for shallow parsing. Fortunately, for English and most extensively spoken languages, such data and pretrained models exist. Example 2-5 presents an example of shallow parsing using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary - NP\n",
      "the green witch - NP\n"
     ]
    }
   ],
   "source": [
    "doc  = nlp(u\"Mary slapped the green witch.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f'{chunk} - {chunk.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of span that’s useful is the named entity. A named entity is a string mention of a real-world concept like a person, location, organization, drug name, and so on. Here’s an example:\n",
    "\n",
    "![](https://learning.oreilly.com/library/view/natural-language-processing/9781491978221/assets/nlpp_02_NER.png)\n",
    "\n",
    "## Structure of Sentences\n",
    "Whereas shallow parsing identifies phrasal units, the task of identifying the relationship between them is called parsing. You might recall from elementary English class diagramming sentences like in the example shown in Figure 2-3.\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 600px\">\n",
    "        <img src=\"imgs/nlpp_0203.png\">\n",
    "        <h4 style=\"font-family: courier; font-size: .8em;\">Figure 2-3. A constituent parse of the sentence “Mary slapped the green witch.”</h4>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Parse trees indicate how different grammatical units in a sentence are related hierarchically. The parse tree in Figure 2-3 shows what’s called a constituent parse. Another, possibly more useful, way to show relationships is using dependency parsing, depicted in Figure 2-4.\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 600px\">\n",
    "        <img src=\"imgs/nlpp_0204.png\">\n",
    "        <h4 style=\"font-family: courier; font-size: .8em;\">Figure 2-4. A dependency parse of the sentence “Mary slapped the green witch.”</h4>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "To learn more about traditional parsing, see the “References” section at the end of this chapter.\n",
    "\n",
    "## Word Senses and Semantics\n",
    "\n",
    "Words have meanings, and often more than one. The different meanings of a word are called its senses. WordNet, a long-running lexical resource project from Princeton University, aims to catalog the senses of all (well, most) words in the English language, along with other lexical relationships.4 For example, consider a word like “plane.” Figure 2-5 shows the different senses in which this word could be used.\n",
    "\n",
    "<div align=\"center\" style=\"width: 100%;\">\n",
    "    <div align=\"center\" style=\"width: 600px\">\n",
    "        <img src=\"imgs/nlpp_0205.png\">\n",
    "        <h4 style=\"font-family: courier; font-size: .8em;\">Figure 2-5. Senses for the word “plane” (courtesy of WordNet).</h4>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "The decades of effort that have been put into projects like WordNet are worth availing yourself of, even in the presence of modern approaches. Later chapters in this book present examples of using existing linguistic resources in the context of neural networks and deep learning methods.\n",
    "\n",
    "Word senses can also be induced from the context—automatic discovery of word senses from text was actually the first place semi-supervised learning was applied to NLP. Even though we don’t cover that in this book, we encourage you to read Jurafsky and Martin (2014), Chapter 17, and Manning and Schütze (1999), Chapter 7.\n",
    "\n",
    "# Summary\n",
    "In this chapter, we reviewed some basic terminology and ideas in NLP that should be handy in future chapters. This chapter covered only a smattering of what traditional NLP has to offer. We omitted significant aspects of traditional NLP because we want to allocate the bulk of this book to the use of deep learning for NLP. It is, however, important to know that there is a rich body of NLP research work that doesn’t use neural networks, and yet is highly impactful (i.e., used extensively in building production systems). The neural network–based approaches should be considered, in many cases, as a supplement and not a replacement for traditional methods. Experienced practitioners often use the best of both worlds to build state-of-the-art systems. To learn more about the traditional approaches to NLP, we recommend the references listed in the following section.\n",
    "\n",
    "# References\n",
    "\n",
    "1. **Manning, Christopher D., and Hinrich Schütze. (1999). Foundations of Statistical Natural Language Processing. MIT press.**\n",
    "2. **Bird, Steven, Ewan Klein, and Edward Loper. (2009). Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O’Reilly.**\n",
    "3. **Smith, Noah A. (2011). Linguistic Structure prediction. Morgan and Claypool.**\n",
    "4. **Jurafsky, Dan, and James H. Martin. (2014). Speech and Language Processing, Vol. 3. Pearson.**\n",
    "5. **Russell, Stuart J., and Peter Norvig. (2016). Artificial Intelligence: A Modern Approach. Pearson.**\n",
    "6. **Zheng, Alice, and Casari, Amanda. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. O’Reilly.**\n",
    "\n",
    "---\n",
    "\n",
    "1. Translation: “Mary slapped the green witch.” We use this sentence as a running example in this chapter. We acknowledge the example is rather violent, but our use is a hat-tip to the most famous artificial intelligence textbook of our times (Russell and Norvig, 2016), which also uses this sentence as a running example.\n",
    "2. In Chapters 4 and 6, we look at deep learning models that implicitly capture this substructure efficiently.\n",
    "3. To understand the difference between stemming and lemmatization, consider the word “geese.” Lemmatization produces “goose,” whereas stemming produces “gees.”\n",
    "4. Attempts to create multilingual versions of WordNet exist. See BabelNet as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('development')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb57a83638ea7f327316302e899ed2cb49af2abee5efd5bafbc25b049f2b02bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
